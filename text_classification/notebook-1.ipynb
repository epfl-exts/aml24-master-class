{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup Google Colab by running this cell only once (ignore this if run locally) {display-mode: \"form\"}\n",
    "import sys \n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/epfl-exts/aml24-master-class.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"aml24-master-class/text_classification/data\" \"aml24-master-class/text_classification/tools.py\" .\n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"aml24-master-class/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text classification :: Overview\n",
    "\n",
    "### Task \n",
    "\n",
    "We want to build a Spam detector which, given examples of spam emails (e.g. flagged by users) and examples of regular (non-spam, also called \"ham\") emails, learns how to flag new unseen emails as spam or non-spam.\n",
    "\n",
    "### Data\n",
    "\n",
    "We will use the [SpamAssassin](https://spamassassin.apache.org/) public email corpus. This dataset contains ~6'000 labeled emails with a ~30% spam ratio. If you want to learn more about this dataset, check [this](https://spamassassin.apache.org/old/publiccorpus/). (*Note: Datasets of text are called corpora and samples are called documents.*) \n",
    "\n",
    "The dataset has been downloaded for you and is available in the *data* folder.\n",
    "\n",
    "### Notebook overview\n",
    "\n",
    "* Load the data\n",
    "* Text preprocessing\n",
    "* Data exploration\n",
    "* Feature extraction\n",
    "* Build a spam detector\n",
    "* What did our model learn? Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification :: Spam detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and helper functions\n",
    "%run tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of samples per class in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_frequency(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at a few rows from the dataset.\n",
    "\n",
    "***Note:*** The *label* is 0 for *non-spam* and 1 for *spam*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you rerun this cell then you get a different set of samples displayed\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Good text preprocessing is an essential part of every NLP project!\n",
    "\n",
    "Our goal here is to build a model that distinguishes non-spam from spam. The idea here is to \"clean\" and \"standardize\" raw text before feeding it to our machine learning model. We need to keep as many \"informative\" words as possible, while discarding the \"uniformative\" ones. Removing unnecessary content, i.e. the \"noise\", from our texts will help to improve the accuracy of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Observations\n",
    "\n",
    "- There are some items in the text that should be removed to make it readable. Here are some suggestions:\n",
    "\n",
    "* HTML tags \n",
    "* URLs\n",
    "* E-mail addresses\n",
    "* Punctuation marks, digits (e.g. 2002, 1.1, ...)\n",
    "* Multiple whitespaces\n",
    "* Case conversion (e.g. Dog vs dog, ...)\n",
    "* English STOPWORDS (e.g. a, is, my, i, all, and, by...)\n",
    "* ...\n",
    "\n",
    "- It is likely that the number of occurrences of the above items (HTML tags, URLs, etc) is helpful to distinguish spam from non-spam. Similarly, the length of the emails and the frequency of punctuation marks or upper case letters may also give us clues as to whether we are dealing with spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *clean_corpus* function below will take care of the parts raised in the 1st observation. For the ideas from the 2nd observation we will create new features and investigate their effects in the subsection **What about \"spammish\" signatures?**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_corpus(df)\n",
    "\n",
    "print(\"Data cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a few \"cleaned\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_clean_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration :: What makes spam distinct?\n",
    "\n",
    "### Frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words distinguish spam from non-spam? Can we  identify the words in a text that are the most informative about its topic?\n",
    "\n",
    "Let's find the 10 most frequent words in spam and non-spam and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_common_words(df=df, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Observations\n",
    "\n",
    "**Frequent \"spammish\" words**: \n",
    "\n",
    "* free\n",
    "* email\n",
    "* click\n",
    "* business\n",
    "* money\n",
    "\n",
    "**Frequent \"non-spammish\" words**:\n",
    "\n",
    "* just\n",
    "* like\n",
    "* linux\n",
    "* wrote\n",
    "* users  \n",
    "\n",
    "**Occur in both top 10 but could be useful for distinctions**:\n",
    "\n",
    "* list\n",
    "* time\n",
    "\n",
    "**Occur in both top 10 but are unlikely to be useful**:\n",
    "\n",
    "* people\n",
    "* mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_common_words(df=df, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Observations\n",
    "\n",
    "As we use more top words we get more overlap between the classes.  \n",
    "However words like _email_ or _free_ are still mch more frequent in the **spam** class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "Let's change `N=10` to `N=20` and compare the outcome.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about \"spammish\" signatures?\n",
    "\n",
    "* Do spams contain more HTML tags? \n",
    "* Does non-spam contain more URLs and E-mail adresses? \n",
    "* Are spams mails longer than non-spam? \n",
    "* ...\n",
    "\n",
    "Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering :: Extracting features from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers don't understand natural language. So, how do we represent text?\n",
    "\n",
    "One of the simplest but effective and commonly used models to represent text for machine learning is the ***Bag of Words*** model ([online documentation](https://en.wikipedia.org/wiki/Bag-of-words_model)). When using this model, we discard most of the structure of the input text (word order, chapters, paragraphs, sentences and formating) and only count how often each word appears in each text. Discarding the structure and counting only word occurencies leads to the mental image of representing text as a \"bag\".  \n",
    "\n",
    "**Example:** Let our toy corpus contain four documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ corpus = ['I\\;enjoy\\;paragliding.',  $  \n",
    "$\\hspace{2cm}'I\\;like\\;NLP.',$  \n",
    "$\\hspace{2cm}'I\\;like\\;deep\\;learning.',$  \n",
    "$\\hspace{2cm}'O\\;Captain!\\;my\\;Captain!']$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bag_of_words_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words has converted all documents into numeric vectors. Each column represents a word from the corpus and each row one of the four documents. The value in each cell represents the number of times that word appears in a specific document. For example, the fourth document has the word `captain` occuring twice and the words `my` and `O` occuring once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a spam detector\n",
    "\n",
    "In the previous section, we saw how to perform text preprocessing and feature extraction from text. We are now ready to build our machine learning model for detecting spams. We will use a Logistic Regression classifier.\n",
    "\n",
    "First, split the data into two sets: the `train` set and the `test` set. We will then use the train set to `fit` our model. We will use 5-fold cross-validation. So the validation sets are automatically created internally. The test set will be used to `evaluate` the performance of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "70.3% of samples are non-spam. This naive baseline model would reach 70% for doing very little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = train_test_split_(df)\n",
    "\n",
    "# Fit model on the train data\n",
    "model = fit_model(df_train)\n",
    "\n",
    "# Print predictions on test set\n",
    "plot_confusion_matrix(df_test, model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrices**  \n",
    "\n",
    "Confusion matrices are a nice way of evaluating the performance of models for classification models. Rows correspond to the true classes and the columns to the predicted classes. Entries on the main diagonal of the confusion matrix correspond to correct predictions while the other cells tell us how many mistakes made our model ([online documentation](https://en.wikipedia.org/wiki/Confusion_matrix)).\n",
    "\n",
    "* The first row represents non-spam mails: 1'192 were correctly classified as 'non-spam', while 24 (~1,9%) were misclassified as 'spam'.\n",
    "* The second row represents spam mails: 435 were correctly classified as 'spam', while 15 (~3,3%) were misclassified as 'non-spam'.\n",
    "\n",
    "Our model did quite well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did our model learn from the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression model has learned which words are the most indicative of non-spam and which words are the most indicative of spam. The positive coefficients on the right correspond to words that, according to the model, are indicative of spam. The negative coefficients on the left correspond to words that, according to the model, are indicative of non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coefficients(model, n_top_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Observations\n",
    "\n",
    "- According to the model, words such as \"date\", \"wrote\", \"yahoo\", \"supplied\", ... are strong indicators of non-spam.  \n",
    "\n",
    "- Words such as \"click\", \"removed\", \"sightings\",  ... indicate spam.\n",
    "\n",
    "- These results are consistent with our earlier analysis. For example we had identified \"wrote\", \"said\" and \"linux\" as potential indicators of non-spam ealier. Similarly \"click\", \"credit\", \"free\", and \"money\" suggested spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis :: Where does our model fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now analyze the misclassified mails in order to get some insights on where the model failed to make correct predictions. The *error_analysis* function below will show us the top features responsible for the model making a decision of prediction whether the mail is spam or non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(df_test, model, doc_nbr=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-adsml]",
   "language": "python",
   "name": "conda-env-miniconda3-adsml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
